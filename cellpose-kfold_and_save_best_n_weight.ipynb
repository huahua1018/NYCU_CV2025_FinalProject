{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 修改 fold_idx 選擇要做哪一fold (0~4)\n",
    "# datasets 平均分好 5 fold ㄌ\n",
    "fold_idx = '0' # default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T16:50:30.096819Z",
     "iopub.status.busy": "2025-05-31T16:50:30.096609Z",
     "iopub.status.idle": "2025-05-31T16:51:46.910876Z",
     "shell.execute_reply": "2025-05-31T16:51:46.910152Z",
     "shell.execute_reply.started": "2025-05-31T16:50:30.096802Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install cellpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-31T17:28:16.434518Z",
     "iopub.status.busy": "2025-05-31T17:28:16.433908Z",
     "iopub.status.idle": "2025-05-31T17:28:16.444614Z",
     "shell.execute_reply": "2025-05-31T17:28:16.444069Z",
     "shell.execute_reply.started": "2025-05-31T17:28:16.434494Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "augment.py\n",
    "------------------------------------------------------------------\n",
    "• Patch‑wise Mosaic\n",
    "• 階段後：水平/垂直翻轉 + 亮度 ±10%\n",
    "\n",
    "適用 Sartorius 細胞實例分割\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from typing import Sequence, Tuple, Union\n",
    "\n",
    "# ------------ 翻轉 + 亮度流水線 ------------------------------\n",
    "_geom_tf = A.Compose(\n",
    "    [\n",
    "        A.HorizontalFlip(p=0.0),\n",
    "        A.VerticalFlip(p=0.0),\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=0.10,  # ±10% 亮度\n",
    "            contrast_limit=0.00,\n",
    "            p=0.0\n",
    "        ),\n",
    "    ],\n",
    "    additional_targets={\"mask\": \"mask\"},\n",
    "    is_check_shapes=False,\n",
    ")\n",
    "\n",
    "# ------------ Augmenter 類別 ----------------------------------------\n",
    "class Augmenter:\n",
    "    \"\"\"\n",
    "    支援：\n",
    "      1) Patch‑wise Mosaic\n",
    "      2) 水平/垂直翻轉 + 亮度 ±10%\n",
    "\n",
    "    參數:\n",
    "      mosaic_prob: Mosaic 執行機率 (0~1)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        mosaic_prob: float = 0.0\n",
    "    ):\n",
    "        self.mosaic_prob = mosaic_prob\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        imgs: Union[np.ndarray, Sequence[np.ndarray]],\n",
    "        masks: Union[np.ndarray, Sequence[np.ndarray]],\n",
    "        epoch: int = 0\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        # 支援單張或 batch\n",
    "        single = False\n",
    "        if isinstance(imgs, np.ndarray) and imgs.ndim == 3:\n",
    "            imgs_list = [imgs]\n",
    "            masks_list = [masks]  # type: ignore\n",
    "            single = True\n",
    "        else:\n",
    "            imgs_list = list(imgs)  # type: ignore\n",
    "            masks_list = list(masks)  # type: ignore\n",
    "\n",
    "        out_imgs, out_masks = [], []\n",
    "        N = len(imgs_list)\n",
    "        for idx, (im, mk) in enumerate(zip(imgs_list, masks_list)):\n",
    "            # 1) 翻轉 + 亮度\n",
    "            aug = _geom_tf(image=im, mask=mk)\n",
    "            out_imgs.append(aug[\"image\"].astype(im.dtype))\n",
    "            out_masks.append(aug[\"mask\"].astype(mk.dtype))\n",
    "\n",
    "        if single:\n",
    "            return out_imgs[0], out_masks[0]\n",
    "        return np.stack(out_imgs), np.stack(out_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T17:28:22.774671Z",
     "iopub.status.busy": "2025-05-31T17:28:22.774424Z",
     "iopub.status.idle": "2025-05-31T17:28:40.186306Z",
     "shell.execute_reply": "2025-05-31T17:28:40.185551Z",
     "shell.execute_reply.started": "2025-05-31T17:28:22.774653Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time, logging\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler          # ← AMP\n",
    "from cellpose import models, train\n",
    "from cellpose.transforms import random_rotate_and_resize\n",
    "\n",
    "train_logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "#  Train with callback  +  AMP support\n",
    "# ---------------------------------------------------------------------------\n",
    "def train_seg_with_callback(\n",
    "        net,\n",
    "        train_data=None, train_labels=None, train_files=None,\n",
    "        train_labels_files=None, train_probs=None,\n",
    "        test_data=None,  test_labels=None,  test_files=None,\n",
    "        test_labels_files=None, test_probs=None,\n",
    "        channel_axis=None, load_files=True,\n",
    "        batch_size=1, learning_rate=5e-5, SGD=False,\n",
    "        n_epochs=100, weight_decay=0.1,\n",
    "        normalize=True, compute_flows=False,\n",
    "        save_path=None, save_every=100, save_each=False,\n",
    "        nimg_per_epoch=None, nimg_test_per_epoch=None,\n",
    "        rescale=False, scale_range=None, bsize=256,\n",
    "        min_train_masks=5, model_name=None, class_weights=None,\n",
    "        callback=None, validate_every=5, tb_writer=None,\n",
    "        augment_fn=None, use_amp=True  # ← 新增\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Cellpose segmentation training with:\n",
    "      • custom callback\n",
    "      • AMP half-precision (set use_amp=False to disable)\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------- basic setup ----------------------- #\n",
    "    if SGD:\n",
    "        train_logger.warning(\"SGD is deprecated, using AdamW instead\")\n",
    "\n",
    "    device = net.device\n",
    "    scale_range = 0.5 if scale_range is None else scale_range\n",
    "\n",
    "    # normalisation dict handling\n",
    "    if isinstance(normalize, dict):\n",
    "        normalize_params = {**models.normalize_default, **normalize}\n",
    "    elif isinstance(normalize, bool):\n",
    "        normalize_params = models.normalize_default\n",
    "        normalize_params[\"normalize\"] = normalize\n",
    "    else:\n",
    "        raise ValueError(\"normalize must be bool or dict\")\n",
    "\n",
    "    # preprocess data (cellpose util)\n",
    "    result = train._process_train_test(\n",
    "        train_data=train_data, train_labels=train_labels,\n",
    "        train_files=train_files, train_labels_files=train_labels_files,\n",
    "        train_probs=train_probs,\n",
    "        test_data=test_data, test_labels=test_labels,\n",
    "        test_files=test_files, test_labels_files=test_labels_files,\n",
    "        test_probs=test_probs,\n",
    "        load_files=load_files, min_train_masks=min_train_masks,\n",
    "        compute_flows=compute_flows, channel_axis=channel_axis,\n",
    "        normalize_params=normalize_params, device=device\n",
    "    )\n",
    "    (train_data, train_labels, train_files, train_labels_files, train_probs, diam_train,\n",
    "     test_data,  test_labels,  test_files,  test_labels_files, test_probs, diam_test,\n",
    "     normed) = result\n",
    "\n",
    "    # parameters for later batches\n",
    "    kwargs = {} if normed else {\"normalize_params\": normalize_params,\n",
    "                                \"channel_axis\": channel_axis}\n",
    "\n",
    "    net.diam_labels.data = torch.Tensor([diam_train.mean()]).to(device)\n",
    "\n",
    "    # optional class weights\n",
    "    if class_weights is not None and isinstance(class_weights, (list, np.ndarray, tuple)):\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float32,\n",
    "                                     device=device)\n",
    "\n",
    "    # dataset sizes\n",
    "    nimg        = len(train_data) if train_data is not None else len(train_files)\n",
    "    nimg_test   = len(test_data) if test_data is not None else len(test_files) if test_files is not None else None\n",
    "    nimg_per_epoch      = nimg       if nimg_per_epoch       is None else nimg_per_epoch\n",
    "    nimg_test_per_epoch = nimg_test  if nimg_test_per_epoch  is None else nimg_test_per_epoch\n",
    "\n",
    "    # ----------------------- lr schedule ---------------------- #\n",
    "    warmup_epochs = 5                   # ← 5 個 warm-up\n",
    "    base_lr       = learning_rate       # = 1e-5\n",
    "    eta_min       = 1e-7                # 最低 LR\n",
    "\n",
    "    LR = np.zeros(n_epochs, dtype=np.float32)\n",
    "\n",
    "    # ❶ 線性 warm-up: 0 → base_lr\n",
    "    for e in range(min(warmup_epochs, n_epochs)):\n",
    "        LR[e] = base_lr * (e + 1) / warmup_epochs\n",
    "\n",
    "    # ❷ Cosine phase\n",
    "    if n_epochs > warmup_epochs:\n",
    "        t = np.arange(0, n_epochs - warmup_epochs)\n",
    "        cos_part = eta_min + 0.5 * (base_lr - eta_min) * \\\n",
    "                (1 + np.cos(np.pi * t / (n_epochs - warmup_epochs)))\n",
    "        LR[warmup_epochs:] = cos_part\n",
    "\n",
    "    # ----------------------- optimizer & scaler --------------- #\n",
    "    optimizer = torch.optim.AdamW(net.parameters(),\n",
    "                                  lr=learning_rate,\n",
    "                                  weight_decay=weight_decay)\n",
    "    scaler = GradScaler(enabled=use_amp)          # AMP scaler\n",
    "\n",
    "    # ----------------------- paths ----------------------------- #\n",
    "    t0 = time.time()\n",
    "    model_name = f\"cellpose_{t0}\" if model_name is None else model_name\n",
    "    save_path  = Path.cwd() if save_path is None else Path(save_path)\n",
    "    model_dir  = save_path / model_name\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "    filename   = model_dir / model_name\n",
    "    train_logger.info(f\"Saving checkpoints to {filename}\")\n",
    "\n",
    "    # ----------------------- track losses ---------------------- #\n",
    "    train_losses = np.zeros(n_epochs, dtype=np.float32)\n",
    "    test_losses  = np.zeros(n_epochs, dtype=np.float32)\n",
    "\n",
    "    # ----------------------- epoch loop ------------------------ #\n",
    "    for iepoch in trange(n_epochs, desc=\"Epoch\", ncols=100):\n",
    "        # set seed & shuffle\n",
    "        np.random.seed(iepoch)\n",
    "        rperm = (np.random.choice(np.arange(nimg), nimg_per_epoch, p=train_probs)\n",
    "                 if nimg != nimg_per_epoch\n",
    "                 else np.random.permutation(np.arange(nimg)))\n",
    "\n",
    "        # update LR\n",
    "        for pg in optimizer.param_groups:\n",
    "            pg[\"lr\"] = LR[iepoch]\n",
    "\n",
    "        net.train()\n",
    "        epoch_train_loss, nsamples = 0.0, 0\n",
    "\n",
    "        # -------- mini-batch loop -------- #\n",
    "        for k in trange(0, nimg_per_epoch, batch_size,\n",
    "                        leave=False, desc=\"Train\", ncols=100):\n",
    "            inds = rperm[k:k+batch_size]\n",
    "            imgs, lbls = train._get_batch(inds,\n",
    "                                          data=train_data, labels=train_labels,\n",
    "                                          files=train_files, labels_files=train_labels_files,\n",
    "                                          **kwargs)\n",
    "            diams = np.array([diam_train[i] for i in inds])\n",
    "            rsc   = diams / net.diam_mean.item() if rescale else np.ones_like(diams)\n",
    "\n",
    "            if augment_fn is not None:\n",
    "                imgs, lbls = augment_fn(imgs, lbls, epoch=iepoch)\n",
    "\n",
    "            imgi, lbl = random_rotate_and_resize(imgs, Y=lbls,\n",
    "                                                 rescale=rsc,\n",
    "                                                 scale_range=scale_range,\n",
    "                                                 xy=(bsize, bsize))[:2]\n",
    "\n",
    "            X   = torch.from_numpy(imgi).to(device)\n",
    "            lbl = torch.from_numpy(lbl).to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with autocast(enabled=use_amp):\n",
    "                y = net(X)[0]\n",
    "                loss = train._loss_fn_seg(lbl, y, device)\n",
    "                if y.shape[1] > 3:\n",
    "                    loss += train._loss_fn_class(lbl, y,\n",
    "                                                 class_weights=class_weights)\n",
    "\n",
    "            # ------ backward scaled ------ #\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            batch_loss = loss.item() * len(imgi)\n",
    "            epoch_train_loss += batch_loss\n",
    "            nsamples         += len(imgi)\n",
    "\n",
    "        train_losses[iepoch] = epoch_train_loss / nsamples\n",
    "\n",
    "        # -------- validation -------- #\n",
    "        val_loss_mean = 0.0\n",
    "        if (test_data is not None or test_files is not None) and \\\n",
    "           (iepoch % validate_every == 0 or iepoch == n_epochs-1):\n",
    "\n",
    "            np.random.seed(42)\n",
    "            rperm = (np.random.choice(np.arange(nimg_test), nimg_test_per_epoch, p=test_probs)\n",
    "                     if nimg_test != nimg_test_per_epoch\n",
    "                     else np.random.permutation(np.arange(nimg_test)))\n",
    "\n",
    "            net.eval()\n",
    "            val_sum, v_nsamp = 0.0, 0\n",
    "            with torch.no_grad():\n",
    "                for k in trange(0, len(rperm), batch_size,\n",
    "                                leave=False, desc=\"Val\", ncols=100):\n",
    "                    inds = rperm[k:k+batch_size]\n",
    "                    imgs, lbls = train._get_batch(inds,\n",
    "                                                  data=test_data, labels=test_labels,\n",
    "                                                  files=test_files, labels_files=test_labels_files,\n",
    "                                                  **kwargs)\n",
    "                    diams = np.array([diam_test[i] for i in inds])\n",
    "                    rsc   = diams / net.diam_mean.item() if rescale else np.ones_like(diams)\n",
    "\n",
    "                    imgi, lbl = random_rotate_and_resize(imgs, Y=lbls,\n",
    "                                                         rescale=rsc,\n",
    "                                                         scale_range=scale_range,\n",
    "                                                         xy=(bsize, bsize))[:2]\n",
    "                    X   = torch.from_numpy(imgi).to(device)\n",
    "                    lbl = torch.from_numpy(lbl).to(device)\n",
    "\n",
    "                    with autocast(enabled=use_amp):\n",
    "                        y = net(X)[0]\n",
    "                        vloss = train._loss_fn_seg(lbl, y, device)\n",
    "                        if y.shape[1] > 3:\n",
    "                            vloss += train._loss_fn_class(lbl, y,\n",
    "                                                          class_weights=class_weights)\n",
    "                    val_sum  += vloss.item() * len(imgi)\n",
    "                    v_nsamp  += len(imgi)\n",
    "\n",
    "            val_loss_mean           = val_sum / v_nsamp\n",
    "            test_losses[iepoch]     = val_loss_mean\n",
    "        else:\n",
    "            test_losses[iepoch] = test_losses[iepoch-1] if iepoch else 0\n",
    "\n",
    "        # -------- callback / TB log -------- #\n",
    "        if callback is not None:\n",
    "            callback.on_epoch_end(iepoch,\n",
    "                                  train_losses[iepoch],\n",
    "                                  test_losses[iepoch],\n",
    "                                  LR[iepoch])\n",
    "\n",
    "        if tb_writer is not None:\n",
    "            tb_writer.add_scalar(\"Loss/train\", train_losses[iepoch], iepoch)\n",
    "            tb_writer.add_scalar(\"Loss/valid\", test_losses[iepoch],  iepoch)\n",
    "            tb_writer.add_scalar(\"LR\",         LR[iepoch],          iepoch)\n",
    "\n",
    "        # -------- save checkpoint -------- #\n",
    "        if iepoch == n_epochs-1 or (iepoch % save_every == 0 and iepoch):\n",
    "            ckpt_name = (f\"{filename}_epoch_{iepoch:04d}\"\n",
    "                         if (save_each and iepoch != n_epochs-1) else filename)\n",
    "            net.save_model(str(ckpt_name))\n",
    "            train_logger.info(f\"Saved model to {ckpt_name}\")\n",
    "\n",
    "    # final save\n",
    "    net.save_model(str(filename))\n",
    "    train_logger.info(f\"Final model saved to {filename}\")\n",
    "    return str(filename), train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-31T17:45:41.054Z",
     "iopub.execute_input": "2025-05-31T17:34:01.154805Z",
     "iopub.status.busy": "2025-05-31T17:34:01.154532Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from cellpose import io, models, core, metrics\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "log_dir_base = Path(\"/kaggle/working/runs\")\n",
    "log_dir_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# --------------------------- environment setup ----------------------------- #\n",
    "io.logger_setup()\n",
    "if not core.use_gpu():\n",
    "    raise ImportError(\"No GPU detected; please switch to a GPU runtime\")\n",
    "\n",
    "model_name = \"fold\" + fold_idx\n",
    "save_path = Path(\"/kaggle/working\")\n",
    "save_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Training hyperparameters\n",
    "n_epochs      = 100\n",
    "learning_rate = 1e-5\n",
    "weight_decay  = 0.1\n",
    "batch_size    = 2\n",
    "\n",
    "train_dir = Path(\"/kaggle/input/fold-data/mytrain_fold\" + fold_idx)\n",
    "test_dir  = Path(\"/kaggle/input/fold-data/myval_fold\" + fold_idx)\n",
    "masks_ext = \"_seg.npy\"\n",
    "\n",
    "if not train_dir.exists() or not test_dir.exists():\n",
    "    raise FileNotFoundError(\"train_dir or test_dir not found\")\n",
    "\n",
    "# --------------------------- load full-size images & masks ----------------------------- #\n",
    "train_imgs, train_msks, _, val_imgs, val_msks, _ = io.load_train_test_data(\n",
    "    str(train_dir), str(test_dir),\n",
    "    mask_filter=masks_ext,\n",
    "    image_filter=\"\"\n",
    ")\n",
    "print(f\"Loaded full images → train: {len(train_imgs)}, val: {len(val_imgs)}\")\n",
    "\n",
    "# --------------------------------- 不作 patch 切割，直接使用原圖和原遮罩 --------------------------\n",
    "train_data, train_labels = train_imgs, train_msks\n",
    "val_data,   val_labels  = val_imgs,   val_msks\n",
    "\n",
    "# --------------------------- 確認 GPU 可用性 ----------------------------- #\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ---------------------- 初始化 Augmenter、並在 50 epoch 後關閉 Mosaic ----------------------\n",
    "# 由於現在不做 patch-based mosaic，若只想要原本的翻轉/亮度等等強度 Augment，可自行調整 augment_fn\n",
    "train_aug = Augmenter(mosaic_prob=0.0)\n",
    "\n",
    "def scheduled_augment(imgs, masks, epoch=0):\n",
    "    # 這裡維持「不做任何 mosaic」的設計，你可視需要在 epoch < 50 時做其他 augment\n",
    "    if epoch >= 0:\n",
    "        return imgs, masks\n",
    "    return train_aug(imgs, masks, epoch=epoch)\n",
    "\n",
    "# --------------------------- set up TensorBoard callback ----------------------------- #\n",
    "log_dir = Path(\"/kaggle/working/runs\") / model_name\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "writer = SummaryWriter(str(log_dir))\n",
    "\n",
    "class MAPCallback:\n",
    "    \"\"\"\n",
    "    Custom callback to log training/validation loss, learning rate,\n",
    "    並計算 full-res 驗證集上的 mAP@0.50 及 mAP@[0.50:0.95]。\n",
    "    \"\"\"\n",
    "    def __init__(self, model, val_imgs, val_msks, writer, save_path, model_name, num_best_models=5):\n",
    "        self.model    = model\n",
    "        self.val_imgs = val_imgs\n",
    "        self.val_msks = val_msks\n",
    "        self.writer   = writer\n",
    "        self.save_path = Path(save_path) # 模型保存的基礎路徑\n",
    "        self.model_name = model_name # 模型基礎名稱\n",
    "        self.num_best_models = num_best_models # 要保留的最佳模型數量\n",
    "        self.best_models = [] # 儲存 (mAP, epoch, temp_model_path) 的列表\n",
    "        \n",
    "    def on_epoch_end(self, epoch, train_loss, val_loss, lr):\n",
    "        # 1) Log losses and LR\n",
    "        self.writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "        self.writer.add_scalar(\"Loss/val\",   val_loss,   epoch)\n",
    "        self.writer.add_scalar(\"LR\",         lr,         epoch)\n",
    "        # 2) Infer on full-res val set\n",
    "        with torch.no_grad():\n",
    "            preds_list, *_ = self.model.eval(\n",
    "                self.val_imgs, batch_size=1, diameter=None\n",
    "            )\n",
    "        \n",
    "        # 3) Compute COCO-style average_precision\n",
    "        ap_array = metrics.average_precision(self.val_msks, preds_list)[0]\n",
    "        \n",
    "        # 4) Log mAP@0.50 and mAP@[0.50:0.95]\n",
    "        self.writer.add_scalar(\"mAP50/val\",    ap_array[:,0].mean(), epoch)\n",
    "        self.writer.add_scalar(\"mAP50_95/val\", ap_array.mean(),      epoch)\n",
    "\n",
    "        current_mAP50_95 = ap_array.mean() if ap_array.size > 0 else 0.0\n",
    "        current_mAP50 = ap_array[:,0].mean() if ap_array.shape[1] > 0 else 0.0\n",
    "        \n",
    "        # 5) 追蹤並保存最佳模型\n",
    "        current_model_filename = self.save_path / \\\n",
    "                                 f\"{self.model_name}_epoch_{epoch:04d}_mAP_{current_mAP50_95:.4f}.pth\"\n",
    "\n",
    "        if len(self.best_models) < self.num_best_models:\n",
    "            # 如果還沒有達到數量限制，直接添加並保存\n",
    "            self.model.net.save_model(str(current_model_filename))\n",
    "            print(f\"Saved model to {current_model_filename}\")\n",
    "            self.best_models.append((current_mAP50_95, epoch, current_model_filename))\n",
    "            self.best_models.sort(key=lambda x: x[0], reverse=True) # 按 mAP 降序排序\n",
    "\n",
    "        elif current_mAP50_95 > self.best_models[-1][0]:\n",
    "            # 如果當前 mAP 比列表中最差的還要好，則替換它\n",
    "            old_mAP, old_epoch, old_path = self.best_models.pop() # 移除最差的\n",
    "            if old_path and old_path.exists():\n",
    "                os.remove(old_path) # 刪除舊模型文件\n",
    "                print(f\"Removed old best model: {old_path.name} (mAP={old_mAP:.4f})\")\n",
    "\n",
    "            # 保存新模型\n",
    "            self.model.net.save_model(str(current_model_filename))\n",
    "            print(f\"Saved new best model: {current_model_filename}\")\n",
    "            \n",
    "            self.best_models.append((current_mAP50_95, epoch, current_model_filename))\n",
    "            self.best_models.sort(key=lambda x: x[0], reverse=True) # 重新排序\n",
    "\n",
    "# --------------------------- build model & register callback ----------------------------- #\n",
    "model = models.CellposeModel(gpu=True, pretrained_model = r'/kaggle/input/cellposesam-pretrained-weight-on-livecell/LiveCell_epoch50_bs2_norm_part2')\n",
    "net   = model.net.to(device)\n",
    "map_cb = MAPCallback(model, val_data, val_labels, writer, save_path=save_path, model_name=model_name, num_best_models=5)\n",
    "\n",
    "# --------------------------- run training ----------------------------- #\n",
    "new_model_path, train_losses, val_losses = train_seg_with_callback(\n",
    "    net,\n",
    "    train_data=train_data,         # 直接用 full-res 圖\n",
    "    train_labels=train_labels,     # 直接用 full-res 遮罩\n",
    "    test_data=val_data,            # full-res 驗證集\n",
    "    test_labels=val_labels,        # full-res 驗證遮罩\n",
    "    batch_size=batch_size,\n",
    "    n_epochs=n_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    nimg_per_epoch=len(train_data),\n",
    "    save_every=n_epochs + 1, # 將 Cellpose 內建的定期保存功能關閉，只在最後保存一個模型\n",
    "    save_each=False, # 確保不會生成大量臨時文件\n",
    "    save_path=save_path, # 這個 save_path 現在主要用於傳遞給 callback 以便在 /kaggle/working/ 下保存\n",
    "    model_name=model_name,\n",
    "    callback=map_cb,\n",
    "    validate_every=1,\n",
    "    min_train_masks=0,\n",
    "    normalize=True,\n",
    "    augment_fn=scheduled_augment,  # ← 使用簡單的 augment_fn\n",
    ")\n",
    "\n",
    "print(\"✅ Training complete. Model saved to:\", new_model_path)\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7561377,
     "sourceId": 12018493,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7561473,
     "sourceId": 12018639,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
